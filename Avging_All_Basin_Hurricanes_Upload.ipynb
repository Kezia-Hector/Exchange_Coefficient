{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kezia-Hector/Exchange_Coefficient/blob/main/Avging_All_Basin_Hurricanes_Upload.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjfyerJqi0_g",
        "outputId": "49fffddf-932f-44a7-c99d-8f8e34196be3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3ePe5YRIZ_q",
        "outputId": "9bc4e93c-efc5-402a-cb43-0a67e0949967"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: netCDF4 in /usr/local/lib/python3.8/dist-packages (1.6.2)\n",
            "Requirement already satisfied: numpy>=1.9 in /usr/local/lib/python3.8/dist-packages (from netCDF4) (1.21.6)\n",
            "Requirement already satisfied: cftime in /usr/local/lib/python3.8/dist-packages (from netCDF4) (1.6.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from scipy) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install netCDF4\n",
        "!pip install scipy\n",
        "import netCDF4 as nc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEsj6pIgbPu4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import netCDF4 as nc\n",
        "import numpy as np\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSh-szqcbVbL"
      },
      "outputs": [],
      "source": [
        "TC_data_dir = '/content/drive/MyDrive/Colab Notebooks/tcdata_netcdf/'#Shared with me/MCFC IFs/Value Chain Resilience/Resiliency UROP/Kezia'\n",
        "basins = ['shtracks','wptracks','attracks','iotracks','eptracks']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UP0lhqDbyEDb"
      },
      "outputs": [],
      "source": [
        "errorstorms = []\n",
        "k=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2DJ8t_K_JjA"
      },
      "outputs": [],
      "source": [
        "for basin in basins:\n",
        "  fn = TC_data_dir+basin+'.nc'\n",
        "  ds = nc.Dataset(fn)\n",
        "  # get list of storm names\n",
        "  sn_data= ds.variables['stnamec'][:]\n",
        "  N_chars_per_name = sn_data.shape[0]\n",
        "  N_dat_pts = sn_data.shape[1]\n",
        "  storm_names = []\n",
        "  for i in range(N_dat_pts):\n",
        "    name_bytes = sn_data.transpose()[i]\n",
        "    name_string = name_bytes.tobytes().decode(\"utf-8\")\n",
        "    storm_names.append(name_string)\n",
        "  #print(storm_names)\n",
        "  fn = TC_data_dir+basin+'.nc'\n",
        "  ds = nc.Dataset(fn)\n",
        "      # see what variables are there\n",
        "      #print(ds.variables.keys())\n",
        "      # look at a specific variable\n",
        "  lat_data= ds.variables['latmc']\n",
        "      # get the data out of the variable\n",
        "  lat = lat_data[:]\n",
        "  lon= ds.variables['longmc'][:] # longitude\n",
        "  month= ds.variables['monthmc'][:] # month\n",
        "  day= ds.variables['daymc'][:] # day\n",
        "  hour= ds.variables['hourmc'][:] # hour\n",
        "  v= ds.variables['vsmc'][:] # speed m/s\n",
        "  year = ds.variables[\"yearic\"][:]\n",
        "  #Database only has from 1959 forward so change the startpt to 1959 rather than the beginning of the dataset\n",
        "  startpt = np.argwhere(np.array(year).transpose() >= 1959)\n",
        "  updated_storm_names = storm_names[startpt[0][0]:]\n",
        "  #Storms we have already looked at\n",
        "  Done_Storm_Names = ['Not_named   ']\n",
        "  if 'Not_named' in storm_names[0]:\n",
        "    Done_Storm_Names.append(storm_names[0])\n",
        "  \n",
        "  #R value using Kerry data\n",
        "  R_Kerry = {}\n",
        "\n",
        "  max_v_dict = {}\n",
        "\n",
        "  for char in updated_storm_names:\n",
        "    if char not in Done_Storm_Names:\n",
        "      match = [i for i, s in enumerate(updated_storm_names) if char in s]\n",
        "      n = len(match)\n",
        "      i=0\n",
        "      Done_Storm_Names.append(char)\n",
        "      while i< n:\n",
        "        fn_Kerry = TC_data_dir+basin+'.nc'\n",
        "        ds_Kerry = nc.Dataset(fn_Kerry)\n",
        "        # see what variables are there\n",
        "        #print(ds.variables.keys())\n",
        "        # look at a specific variable\n",
        "        lat_data= ds_Kerry.variables['latmc']\n",
        "        # get the data out of the variable\n",
        "        lat = lat_data[:]\n",
        "        lon= ds_Kerry.variables['longmc'][:] # longitude\n",
        "        month= ds_Kerry.variables['monthmc'][:] # month\n",
        "        day= ds_Kerry.variables['daymc'][:] # day\n",
        "        hour= ds_Kerry.variables['hourmc'][:] # hour\n",
        "        v= ds_Kerry.variables['vsmc'][:] # speed m/s\n",
        "        year = ds_Kerry.variables[\"yearic\"][:]\n",
        "        Storm_number = match[i]\n",
        "        Storm_lat_array = np.array(lat).transpose()[Storm_number + startpt[0][0]][:]\n",
        "        Storm_lon_array = np.array(lon).transpose()[Storm_number + startpt[0][0]][:]\n",
        "        Storm_month_array = np.array(month).transpose()[Storm_number + startpt[0][0]][:]\n",
        "        Storm_day_array = np.array(day).transpose()[Storm_number + startpt[0][0]][:]\n",
        "        Storm_hour_array = np.array(hour).transpose()[Storm_number + startpt[0][0]][:]\n",
        "        Storm_v_array = np.array(v).transpose()[Storm_number + startpt[0][0]][:]\n",
        "        Storm_year = np.array(year).transpose()[Storm_number + startpt[0][0]]\n",
        "        if Storm_year < 1959.0:\n",
        "          i+=1\n",
        "          continue\n",
        "        else:\n",
        "          max_v = max(Storm_v_array)\n",
        "          max_v_location = np.argwhere(Storm_v_array == max_v)[0][0]\n",
        "          max_v_hour = Storm_hour_array[max_v_location]\n",
        "          max_v_day = Storm_day_array[max_v_location]\n",
        "          max_v_month = Storm_month_array[max_v_location]\n",
        "\n",
        "          try:         \n",
        "            import math\n",
        "            import numpy as np\n",
        "            fn_cds_p = '/content/drive/MyDrive/Colab Notebooks/{0} Hurricane Data/{1}Pressure.nc'.format(basin,char+str(Storm_year))\n",
        "\n",
        "            fn_cds_ee = '/content/drive/MyDrive/Colab Notebooks/{0} Hurricane Data/{1}EverythingElse.nc'.format(basin,char+str(Storm_year))\n",
        "\n",
        "            ds_cds_p = nc.Dataset(fn_cds_p)\n",
        "            ds_cds_ee = nc.Dataset(fn_cds_ee)\n",
        "\n",
        "            u0 = ds_cds_ee['u10'][:]\n",
        "            v0 = ds_cds_ee['v10'][:]\n",
        "            Td0 = ds_cds_ee['d2m'][:]\n",
        "            Ta0 = ds_cds_ee['t2m'][:]\n",
        "            Ts0 = ds_cds_ee['sst'][:]\n",
        "            p0 = ds_cds_ee['sp'][:]\n",
        "            To0 = ds_cds_p['t'][:]\n",
        "            latitude = ds_cds_ee['latitude'][:]\n",
        "            longitude = ds_cds_ee['longitude'][:]\n",
        "\n",
        "\n",
        "\n",
        "            Lv = 2260000.0\n",
        "            cp = 4186.0\n",
        "            a = 17.27 # for Td to q calc\n",
        "            b = 35.86 # for Td to q calc\n",
        "            \n",
        "            max_v_lat = Storm_lat_array[max_v_location]\n",
        "            max_v_lat_round = round(max_v_lat*4)/4\n",
        "\n",
        "\n",
        "            max_v_lon = Storm_lon_array[max_v_location]\n",
        "            max_v_lon_round = round(max_v_lon*4)/4\n",
        "\n",
        "            latitude_start = np.argwhere(latitude == max_v_lat_round - 0.5)[0][0]\n",
        "            latitude_end = np.argwhere(latitude == max_v_lat_round + 0.5)[0][0]\n",
        "\n",
        "            longitude_end = np.argwhere(longitude == max_v_lon_round + 0.5)[0][0]\n",
        "            longitude_start = np.argwhere(longitude == max_v_lon_round - 0.5)[0][0]\n",
        "            \n",
        "            if latitude_end < latitude_start and longitude_start < longitude_end:\n",
        "              Td1 = Td0[0,latitude_end:latitude_start,longitude_start:longitude_end]\n",
        "              Ta1 = Ta0[0,latitude_end:latitude_start,longitude_start:longitude_end]\n",
        "              To1 = To0[0,latitude_end:latitude_start,longitude_start:longitude_end]\n",
        "              Ts1 = Ts0[0,latitude_end:latitude_start,longitude_start:longitude_end]\n",
        "              p1 = p0[0,latitude_end:latitude_start,longitude_start:longitude_end]\n",
        "            if latitude_end > latitude_start and longitude_start < longitude_end:\n",
        "              Td1 = Td0[0,latitude_start:latitude_end,longitude_start:longitude_end]\n",
        "              Ta1 = Ta0[0,latitude_start:latitude_end,longitude_start:longitude_end]\n",
        "              To1 = To0[0,latitude_start:latitude_end,longitude_start:longitude_end]\n",
        "              Ts1 = Ts0[0,latitude_start:latitude_end,longitude_start:longitude_end]\n",
        "              p1 = p0[0,latitude_start:latitude_end,longitude_start:longitude_end]\n",
        "            if latitude_end < latitude_start and longitude_start > longitude_end:\n",
        "              Td1 = Td0[0,latitude_end:latitude_start,longitude_end:longitude_start]\n",
        "              Ta1 = Ta0[0,latitude_end:latitude_start,longitude_end:longitude_start]\n",
        "              To1 = To0[0,latitude_end:latitude_start,longitude_end:longitude_start]\n",
        "              Ts1 = Ts0[0,latitude_end:latitude_start,longitude_end:longitude_start]\n",
        "              p1 = p0[0,latitude_end:latitude_start,longitude_end:longitude_start]\n",
        "            if latitude_end > latitude_start and longitude_start > longitude_end:\n",
        "              Td1 = Td0[0,latitude_start:latitude_end,longitude_end:longitude_start]\n",
        "              Ta1 = Ta0[0,latitude_start:latitude_end,longitude_end:longitude_start]\n",
        "              To1 = To0[0,latitude_start:latitude_end,longitude_end:longitude_start]\n",
        "              Ts1 = Ts0[0,latitude_start:latitude_end,longitude_end:longitude_start]\n",
        "              p1 = p0[0,latitude_start:latitude_end,longitude_end:longitude_start]\n",
        "\n",
        "\n",
        "            Td2 = Td1[Td1>0]\n",
        "            Ta2 = Ta1[Ta1>0]\n",
        "            To2 = To1[To1>0]\n",
        "            Ts2 = Ts1[Ts1>0]\n",
        "            p2 = p1[p1>0]\n",
        "\n",
        "\n",
        "            Td = np.nanmean(Td2)\n",
        "            Ta = np.nanmean(Ta2)\n",
        "            Ts = np.nanmean(Ts2)\n",
        "            To = np.nanmean(To2)\n",
        "            p = np.nanmean(p2)\n",
        "\n",
        "            e = 6.1078*math.exp(a*(Td-273.16)/(Td-b))\n",
        "            qa = 0.622*(e)/(p/100.0-0.378*(e))\n",
        "            es = 6.1078*math.exp(a*(Ts-273.16)/(Ts-b))\n",
        "            qs = 0.622*(es)/(p/100.0-0.378*(es))\n",
        "\n",
        "            #K = k* -k\n",
        "            K= Lv * (qs - qa) + cp*(Ts -Ta)\n",
        "            \n",
        "\n",
        "            Name = basin + char + str(Storm_year)\n",
        "            \n",
        "\n",
        "            #Ratio Calculated using data from Kerry\n",
        "            R_Kerry[Name] = (max_v*max_v * To/(Ts-To))/K \n",
        "\n",
        "        \n",
        "            max_v_dict[Name] = max_v\n",
        "            k+=1\n",
        "            if k%100 == 0:\n",
        "              print(k)\n",
        "          except:\n",
        "            test = Storm_v_array[Storm_v_array!=0]\n",
        "            if len(test) > 0:\n",
        "              errorstorms.append(basin+char+str(Storm_year))\n",
        "          finally:\n",
        "            i+=1\n",
        "      \n",
        "  #Putting Evertything for this basin in the google drive\n",
        "  gc = gspread.authorize(creds)\n",
        "  sh = gc.create('{0} FINAL'.format(basin))\n",
        "\n",
        "  # Open our new sheet and add some data.\n",
        "  worksheet = gc.open('{0} FINAL'.format(basin)).sheet1\n",
        "\n",
        "  names = R_Kerry.keys()\n",
        "  ratios = R_Kerry.values()\n",
        "  maxvs = max_v_dict.values()\n",
        "\n",
        "  z = len(names)\n",
        "\n",
        "  cell_list = worksheet.range('A1:A{0}'.format(z))\n",
        "\n",
        "  i = 0\n",
        "\n",
        "  for cell in cell_list:\n",
        "    cell.value = list(names)[i]\n",
        "    i+=1\n",
        "  worksheet.update_cells(cell_list)\n",
        "\n",
        "  cell_list = worksheet.range('B1:B{0}'.format(z))\n",
        "  i = 0\n",
        "\n",
        "  for cell in cell_list:\n",
        "    cell.value = list(ratios)[i]\n",
        "    i+=1\n",
        "  worksheet.update_cells(cell_list)\n",
        "\n",
        "  cell_list = worksheet.range('C1:C{0}'.format(z))\n",
        "\n",
        "  i = 0\n",
        "\n",
        "  for cell in cell_list:\n",
        "    cell.value = list(maxvs)[i]\n",
        "    i+=1\n",
        "  worksheet.update_cells(cell_list)\n",
        " \n",
        " \n",
        "  print(errorstorms)\n",
        " "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}